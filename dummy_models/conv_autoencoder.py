import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import matplotlib.pyplot as plt

import os
os.chdir('/home/user/Downloads')

# converting data to torch.FloatTensor
transform = transforms.ToTensor()

# download the training and test datasets
train_data = datasets.CIFAR10(root='./CIFAR10',
                              train=True, download=True, transform=transform)
test_data = datasets.CIFAR10(root='./CIFAR10',
                             train=False, download=True, transform=transform)

batch_size = 32

# prepare data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=2)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=2)


# utility functions to un-normalize and display an image
def imshow(img):
    img = img / 2 + 0.5  
    plt.imshow(np.transpose(img, (1, 2, 0))) 

# define the image classes
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck']

# obtain one batch of training images
dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()

# plot the images
fig = plt.figure(figsize=(8, 8))
# display 20 images
for idx in np.arange(9):
    ax = fig.add_subplot(3, 3, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title(classes[labels[idx]])

plt.show()

# define the Convolutional Autoencoder
class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
       
        # ENCODER #
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  
        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
       
        # DECODER #
        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)
        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)


    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = torch.relu(self.t_conv1(x))
        x = torch.sigmoid(self.t_conv2(x)) # layer for BCE loss
              
        return x


# initialize the model
model = ConvAutoencoder().cuda()

# loss function
criterion = nn.BCELoss()

# optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# epochs
n_epochs = 200

for epoch in range(1, n_epochs+1):
    # monitor training loss
    train_loss = 0.0

    # training
    for data in train_loader:
        images, _ = data
        images = images.cuda()
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, images)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()*images.size(0)
          
    train_loss = train_loss/len(train_loader)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, train_loss))


# batch of test images
dataiter = iter(test_loader)
images, labels = dataiter.next()
images, labels = images.cuda(), labels.cuda()

# sample outputs
output = model(images)
images = images.cpu().numpy()

output = output.view(batch_size, 3, 32, 32)
output = output.detach().cpu().numpy()

# original images
fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))
for idx in np.arange(5):
    ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title(classes[labels[idx]])
plt.show()

# reconstructed images
fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))
for idx in np.arange(5):
    ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])
    imshow(output[idx])
    ax.set_title(classes[labels[idx]])
plt.show()